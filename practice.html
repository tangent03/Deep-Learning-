

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
import random
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Load Dataset
data = np.load('mnist.npz')
x_train = data['x_train']
y_train = data['y_train']
x_test  = data['x_test']
y_test  = data['y_test']

# Normalization (divide by dataset max)
mx = x_train.max().astype(np.float32)
x_train = x_train / mx
x_test  = x_test / mx
plt.matshow(x_train[0])

# Build Model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
model.summary()

# Compile Model
model.compile(optimizer='sgd',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# EarlyStopping callback
es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train Model (with EarlyStopping)
history = model.fit(x_train, y_train,
                    epochs=5,                # increase epochs since ES will stop earlier if needed
                    validation_data=(x_test, y_test),
                    callbacks=[es],
                    verbose=1)

# Evaluate Model
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", test_acc)
print("Test loss:", test_loss)

# Prediction on random image
n = random.randint(0, len(x_test) - 1)
plt.imshow(x_test[n])
plt.axis('off')
plt.show()

predicted_value = model.predict(x_test)
print("Handwritten Number in the image is %d" % np.argmax(predicted_value[n]))

# Plot Loss and Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Import Libraries
import numpy as np
import matplotlib.pyplot as plt
import random
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.image import resize


#Load CIFAR-10 Dataset
data  = np.load('cifar-10.npz')
x_train,y_train = data['x_train'],data['y_train']
x_test, y_test = data['x_test'],data['y_test']

#Normalize Pixel Value(0-1 range for faster convergence)
mx = x_train.max().astype(np.float64)
x_train, x_test = x_train/mx, x_test/mx

#Conver class labels into one hot encoding
y_train = keras.utils.to_categorical(y_train,10)
y_test = keras.utils.to_categorical(y_test,10)

print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)

#Model Architecture FNN
model = keras.Sequential([
    layers.Flatten(input_shape=(32,32,3)),
    layers.Dense(512,activation="relu"),
    layers.Dense(256,activation="relu"),
    layers.Dense(128,activation="relu"),
    layers.Dense(10,activation="softmax")
])

#compile.model
sgd = SGD(learning_rate=0.01 , momentum=0.9)
model.compile(optimizer=sgd,loss="categorical_crossentropy",metrics=["accuracy"])
model.summary()

#Early Stopping
es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# --- Train the Model ---
history = model.fit(
    x_train, y_train,
    batch_size=64,
    epochs=20,
    validation_data=(x_test, y_test),
    shuffle=True,
    callbacks=[es]
)

# --- Evaluate Model ---
train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

print(f"Train Accuracy: {train_acc:.2f}, Train Loss: {train_loss:.2f}")
print(f"Test Accuracy: {test_acc:.2f}, Test Loss: {test_loss:.2f}")

# --- Plot Accuracy & Loss ---
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# --- Prediction on Random Image ---
class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']

n = random.randint(0, len(x_test) - 1)
img_resized = resize(x_test[n], (128, 128)).numpy()

plt.imshow(img_resized)
plt.axis('off')
plt.title("True label: " + class_names[np.argmax(y_test[n])])
plt.show()

predicted_value = model.predict(x_test, verbose=0)
print("Predicted label =", class_names[np.argmax(predicted_value[n])])
print("True label =", class_names[np.argmax(y_test[n])])
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from imblearn.over_sampling import SMOTE



data = pd.read_csv("ecg.csv")
print(f"Shape: {data.shape}")
print(f"Target column: {data.columns[-1]}")
X, y = data.iloc[:, :-1], data.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

sm = SMOTE(random_state=42)
X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)

print("Before SMOTE:", np.bincount(y_train))
print("After SMOTE :", np.bincount(y_train_bal))

sns.countplot(x=y)
plt.title("Class Distribution (0 = Normal, 1 = Anomaly)")
plt.show()
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_bal)
X_test_scaled  = scaler.transform(X_test)

print(f"Training dataset shape: {X_train_scaled.shape}, Testing dataset shape: {X_test_scaled.shape}")

input_dim = X_train_scaled.shape[1]
print(f"Input dimension: {input_dim}")
autoencoder = models.Sequential([
    layers.Input(shape=(input_dim,)),

    # Encoder
    layers.Dense(128, activation="relu"),
    layers.Dense(64, activation="relu"),
    layers.Dense(16, activation="tanh"),

    # Decoder
    layers.Dense(64, activation="relu"),
    layers.Dense(128, activation="relu"),
    layers.Dense(input_dim, activation="sigmoid")  
])
autoencoder.compile(
    optimizer="adam",
    loss="mse",
    metrics=["mae"]
)

autoencoder.summary()
history = autoencoder.fit(
    X_train_scaled, X_train_scaled,
    validation_data=(X_test_scaled, X_test_scaled),
    epochs=25, batch_size=256, verbose=1
)

plt.plot(history.history["loss"], label="Train")
plt.plot(history.history["val_loss"], label="Val")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.show()


# Predictions
X_train_pred = autoencoder.predict(X_train_scaled)
X_test_pred  = autoencoder.predict(X_test_scaled)

# Reconstruction error (MSE per sample)
train_errors = np.mean((X_train_scaled - X_train_pred) ** 2, axis=1)
test_errors  = np.mean((X_test_scaled - X_test_pred) ** 2, axis=1)

# Anomaly threshold
threshold = train_errors.mean() + train_errors.std()
print(f"Reconstruction Threshold: {threshold:.4f}")
# Predict anomalies
y_pred = (test_errors > threshold).astype(int)

# Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.2f}")

# Precision
precision = precision_score(y_test, y_pred)
print(f"Precision: {precision:.2f}")

# Recall
recall = recall_score(y_test, y_pred)
print(f"Recall: {recall:.2f}")

# F1 Score
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.2f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#import libraries
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Lambda, Embedding
from tensorflow.keras.utils import to_categorical, pad_sequences
from tensorflow.keras.preprocessing import text, sequence
import tensorflow.keras.backend as K
from sklearn.metrics.pairwise import euclidean_distances


# a. Data Preparation
data = [
    "Natural Language Processing is a field of Artificial Intelligence.",
    "Word embeddings help computers understand human language.",
    "The CBOW model is a part of Word2Vec technique.",
    "CBOW predicts the target word using surrounding context words.",
    "Skip Gram is another architecture of Word2Vec.",
    "Word2Vec is widely used in NLP applications.",
    "Embedding layers in deep learning are used to represent words.",
    "CBOW is faster and works better with frequent words."
]


# Convert sentences to sequences of word IDs

# 2. Tokenize the text
# Tokenizer converts each word into a unique integer ID.
tokenizer = text.Tokenizer()
tokenizer.fit_on_texts(data)

# 3. Build dictionaries
# word2id maps each word to an integer
# id2word maps each integer back to a word
word2id = tokenizer.word_index
word2id['PAD'] = 0                  # add padding token
id2word = {v:k for k,v in word2id.items()}
wids = [[word2id[w] for w in text.text_to_word_sequence(d)] for d in data]

vocab_size = len(word2id)
embed_size = 50
window_size = 2

# ----- b. Generate Training Data -----
# Yield context-target pairs for CBOW
def gen_data(corpus, win, vocab):
    for sent in corpus:
        for i, w in enumerate(sent):
            ctx = [sent[j] for j in range(i-win, i+win+1) if j!=i and 0<=j<len(sent)]
            x = pad_sequences([ctx], maxlen=win*2)  # pad context
            y = to_categorical([w], vocab)          # one-hot target
            yield (x, y)


# ----- c. Build & Train Model -----
# CBOW: Embedding -> Average -> Dense Softmax
model = Sequential([
    Embedding(vocab_size, embed_size, input_shape=(window_size*2,)),
    Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)),
    Dense(vocab_size, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam')


for epoch in range(5):
    loss = sum(model.train_on_batch(x, y) for x, y in gen_data(wids, window_size, vocab_size))
    print(f"Epoch {epoch+1}, Loss: {loss:.2f}")





# ----- d. Output Embeddings & Similar Words -----
# Extract embeddings (skip PAD)
emb = model.get_weights()[0][1:]
pd.DataFrame(emb, index=list(id2word.values())[1:]).head()

# Take user input
words = input("Enter words separated by space: ").lower().split()

# Find top 5 similar words using Euclidean distance
dist = euclidean_distances(emb)
similar = {w: [id2word[i+1] for i in dist[word2id[w]-1].argsort()[1:6]] 
           for w in words if w in word2id}

print("Similar Words:", similar)




                
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



    
